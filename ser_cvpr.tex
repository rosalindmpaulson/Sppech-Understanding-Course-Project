
\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

\cvprfinalcopy

\def\cvprPaperID{****}
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ifcvprfinal\pagestyle{empty}\fi

\begin{document}

\title{Speech Emotion Recognition with CNNs and LLMs}

\author{
Rosalind Margaret Paulson \and Torsha Chatterjee \\
Department of Computer Science, Indian Institute of Technology Jodhpur\\
{\tt\small \{m23csa526, m23csa536\}@iitj.ac.in}
}

\maketitle
\thispagestyle{empty}

\begin{abstract}
Speech emotion recognition (SER) is a critical area within affective computing and human-computer interaction, aiming to identify the emotional state of a speaker based on vocal cues. This paper explores the application of deep learning methods to SER using the CREMA-D dataset. MFCCs are employed to represent each audio sample in a form suitable for machine learning. A CNN is trained to learn discriminative patterns from the MFCCs and perform multi-class classification into six emotion categories: Angry, Disgust, Fear, Happy, Neutral, and Sad. The results demonstrate the potential of MFCC-based CNN architectures in achieving reliable emotion classification from raw audio signals.
\end{abstract}

\section{Introduction}
This study focuses on the development of a speech emotion classification model using the CREMA-D dataset. The objectives are: (1) preprocess and organize the dataset, (2) extract relevant features such as MFCCs, and (3) design and train a CNN for emotion classification. Results show reliable classification with potential for real-world applications.

\section{Methodology}
\subsection{Dataset and Preprocessing}
The CREMA-D dataset consists of 7442 audio clips labeled by emotion. Labels are parsed from filenames, and audio is resampled for consistency.

\subsection{Feature Extraction}
Using the \texttt{librosa} library, MFCCs, RMSE, and ZCR features are extracted. Inputs are padded or truncated to maintain shape uniformity.

\subsection{Label Encoding}
Emotion labels—ANG, DIS, FEA, HAP, NEU, SAD—are one-hot encoded using \texttt{sklearn} and TensorFlow utilities.

\subsection{Model Architecture}
A CNN with two convolutional blocks, max pooling, dropout, and fully connected layers is used. The model ends with a softmax output layer for multi-class classification.

\subsection{Training and Evaluation}
An 80/20 train-test split is used. The CNN is trained for 30 epochs with batch size 32. Performance is evaluated using accuracy and loss.

\section{Analysis}
\subsection{LLM-based Emotion Prediction}
Speech is transcribed and processed via GPT-4o for emotion inference. The pipeline involves: Speech → Text → GPT-4o prompt.

\subsection{CNN+Text Embedding}
Text embeddings from sentence-transformers are combined with audio features. Features are concatenated and fed into a unified model.

\subsection{Data Augmentation}
Three types are applied: additive Gaussian noise, pitch shifting, and combined noise-pitch. These augmentations are used to enhance robustness.

\section{Future Scope}
Potential directions include:
\begin{itemize}
  \item Real-time deployment on edge devices.
  \item Multi-modal fusion with visual and textual data.
  \item Language and speaker independence.
  \item Emotion intensity estimation.
  \item Self-supervised and continual learning.
  \item LLM fine-tuning for emotional inference.
  \item API deployment for integration in real applications.
\end{itemize}

\section{Conclusion}
This study presents a deep learning pipeline for SER using CNNs on MFCC features. Future work may extend to real-time, multilingual, and multimodal setups for more robust emotion-aware systems.

\section*{GitHub}
\noindent \texttt{GitHubLink: [To be added]}

\end{document}
